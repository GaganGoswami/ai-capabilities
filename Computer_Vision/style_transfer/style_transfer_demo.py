






























































































plt.show()plt.imshow(result)result.save('output_image.jpg')result = transforms.ToPILImage()(result)result = target.clone().squeeze()# Save and display the result        print(f'Step {step}, Total loss: {total_loss.item()}')    if step % 50 == 0:        optimizer.step()    total_loss.backward()    optimizer.zero_grad()        total_loss = content_loss_value + style_loss_value * 1e6        style_loss_value = style_loss(target_features)    content_loss_value = content_loss(target_features)        target_features = model(target)for step in range(num_steps):num_steps = 300# Run the style transferoptimizer = optim.Adam([target], lr=0.003)# Define the optimizerstyle_loss = StyleLoss(model(style_image))content_loss = ContentLoss(model(content_image))model = VGG().to(device)# Define the model and lossestarget = content_image.clone().requires_grad_(True)# Initialize the target imagestyle_image = load_image('path_to_style_image.jpg', shape=content_image.shape[-2:])content_image = load_image('path_to_content_image.jpg')# Load the images        return G.div(b * c * h * w)        G = torch.mm(features, features.t())        features = x.view(b * c, h * w)        b, c, h, w = x.size()    def gram_matrix(self, x):            return nn.functional.mse_loss(G, self.target)        G = self.gram_matrix(x)    def forward(self, x):            self.target = self.gram_matrix(target).detach()        super(StyleLoss, self).__init__()    def __init__(self, target):class StyleLoss(nn.Module):        return nn.functional.mse_loss(x, self.target)    def forward(self, x):            self.target = target.detach()        super(ContentLoss, self).__init__()    def __init__(self, target):class ContentLoss(nn.Module):# Define the content and style loss        return self.features(x)    def forward(self, x):            self.features = vgg19(pretrained=True).features[:21]        super(VGG, self).__init__()    def __init__(self):class VGG(nn.Module):# Define the VGG model    return image        image = image.unsqueeze(0)    image = transforms.ToTensor()(image)            image = transforms.Resize(shape)(image)    if shape:            image = transforms.Resize(size)(image)        size = max(max_size, max(image.size))    if max_size:        image = Image.open(image_path).convert('RGB')def load_image(image_path, max_size=400, shape=None):# Load the content and style imagesimport matplotlib.pyplot as pltfrom PIL import Imagefrom torchvision.models import vgg19from torchvision import transformsimport torch.optim as optimimport torch.nn as nnimport torch